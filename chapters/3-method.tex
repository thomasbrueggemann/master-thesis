\section{Implementation and Evaluation of an Automated Information Privacy Risk Assessment Tool}

% Implementation of an Automated Information Privacy Risk Assessment Tool
\subsection{Implementation of an Automated Information Privacy Risk Assessment Tool}

The implementation of an \aiprat is structured in three phases.
In the first phase, Android APK files need to be downloaded to acquire the foundation of a static code analysis: the source code.
While APK files are binary representations of source code, it is necessary, in a second phase, to decompile to binary code back into actual source files.
The third phase is the analysis phase, where the information privacy risk assessment takes place.

Figure \ref{fig:implementationPhases} shows the implementation phases over time including the tools used within each phase. 
The tools will be described in greater detail within the following chapters.

\begin{figure}[h]
	\caption{Diagram of implementation phases of the \aiprat over time.}
	\centering
	\includegraphics[keepaspectratio=true,width=400pt]{figures/ImplementationFlow.png}
	\label{fig:implementationPhases}
\end{figure}

% Download Phase
\subsubsection{Download Phase}

The download phase is the first of the three implementation phases and comprises the acquisition of Android APK files. 
The APK files hold the necessary Java source code that we will perform the \sca on.
Since this thesis emphases on Android mHealth apps, we used the repository database of \cite{Xu2015} as our main app data source.\footnote{This paragraph follows \cite{Xu2015}.}
\cite{Xu2015} list mHealth apps from the Apple AppStore and Android PlayStore and update their repository quarterly by scraping the app stores.
The list contains information for example on the app's id, category in the app stores, description, email address of the developer, price and the user rating of the app.

We used the repository database to loop over the available mHealth app listings and filtered out the apps that were available for free, indicated by a price of \$0.00.
As soon as the package name of an app is gathered, the download of the \acs{APK} file can begin. 
While there is no official source to download \acs{APK} files for Android apps, a multitude of websites exist that host copies of \acs{APK} files to download for free.
Unfortunately, all of these websites implement mechanisms that make it impossible to browse and download the APK files programatically within a download script.
Instead, we used a open source Python implementation of an undocumented Google PlayStore \acs{API}.\footnote{https://github.com/egirault/googleplay-api, visited 05/12/2016} 
The undocumented part of the Google API allows users to download APK files 
Even though the project has not been maintained for four years, the software is still in working order.
The Python script authenticates to the Google API via the hardware ID of an Android smartphone or tablet and pretends to request data from this smartphone or tablet, even though the requests are sent from a desktop computer.
We used a real Android tablet to detect its hardware ID and authenticate the Google API requests with this hardware ID.
The main issue that has to be taken care of during the download phase is not to run into Google API limitations. 
Google allows an API user to only request a certain amount requests per time unit. 
After this limit is exceeded, the requests will just return a HTTP error code and no APK file will be downloaded.
In order to work around this circumstance, we ran our download script multiple times, always until the Google API returns error codes. 
We then waited a couple of hours and tried the download script again, which would pick up the download process where it had stopped on the last run.

% Decompilation Phase
\subsubsection{Decompilation Phase}

In order to decompile the amount of APK files available, it is necessary to automate the process. 
The automation script\footnote{https://github.com/thomasbrueggemann/AIPRAT/blob/master/decompile/decompile.sh} uses a chain of tools to gather access to the source code files from an APK file.
The tools used to decompile the APK files follow closely the tools described and used by \cite{Enck2011}.\footnote{See \cite{Enck2011}, p. 5.}

In a first step, we use the tool \textit{dex2jar} to extract the \acs{JAR} file from the APK file.
The JAR file contains the java bytecode representations of the app which is just one part of the contents of an \acs{APK} file.
The next step is to extract resource files, such as the \textit{Android Manifest} file from the APK file.
The \textit{Android Manifest} contains meta information about the app in a structured XML format.\footnote{This and the next sentence follow \cite{xu2013}, p. 7.}
The meta information include the package name of the app, the permissions the app requests, e.g. camera usage, internet access or geolocation usage.
The \textit{Android Manifest} file is therefore an important indicator for high level activities within the given app.
In order to extract the \textit{Android Manifest} file from the APK file along with other resources such as images, icons, xml files or other files used within the app, we use the \textit{apktool}\footnote{http://ibotpeaches.github.io/Apktool/}.
\textit{apktool} is a frequently updated Android reverse engineering tool that is used to extract resources from APK files.
Another important part of the extraction of resource files is retrieving the layout and localization files.
These files include information on the user interface components used within the app as well as text content for labels and text fields.
The text content will be used to train machine learning algorithms to classify features of the app, further described in the analysis phase section below.

At the core of the decompilation process is the usage of \textit{fernflower}\footnote{https://github.com/fesh0r/fernflower}.
\textit{fernflower} is the recommended java decompiler by \cite{Enck2011}. 
They used the tool to decompile a test sample of apps and gained a significantly higher code recovery rate than by using other decompiling tools.\footnote{See \cite{Enck2011}, p. 6.}
An obstacle in decompiling java source code is obfuscation. 
Java developers can make use of a security feature called obfuscation that aims at hiding away the logic of java classes by renaming classes, variables and method names and disassembling the code into pieces that are difficult to read for an human interpreter.
The idea is to make it more difficult to retrieve and make sense of the original source code by decompiling the byte code.
\textit{fernflower} uses a renaming approach by assigning every obfuscated class with a new naming pattern. 
Member variables and methods will be automatically renamed and therefore provide an easier and more unique way of reading the source code.
Optionally the decompilation process can use an automatic code formatting tool called \textit{astyle}\footnote{http://astyle.sourceforge.net/} to format the source code.
This helps humans to read the source code files more easily, since the formatting and indentation of all source code files is identical and therefore very structured.
Formatting the source code will help in the evaluation phase of this thesis to support the manual inspection the source code for \ipr by human researchers.

The expected result of the decompilation phase is a directory named after the package name of a given app that contains the resource files, including the \textit{Android Manifest} and the decompiled source code of the app.

% Static Code Analysis Phase
\subsubsection{Static Code Analysis Phase} \label{sssec:SCAP}

The \sca phase is the main analysis phase of the thesis and uses the output of the previous decompilation phase to perform the static code analysis.
The \sca tool is implemented as a Java software project, since the used analysis libraries are implemented in Java and Android source code is written in Java too.
The output of the \sca Java project is an executable Java archive file called \textit{AIPRAT.jar} that can be executed in the command line terminal.
In order for \textit{AIPRAT.jar} to perform the \sca on APK files, two preparation steps are required.

The first preparation step is to run an Android data flow analysis tool over the APK files that extract potential data flows.
The data flow analysis is achieved with an open source tool called \textit{FlowDroid}, introduced by \cite{Arzt2014}.\footnote{See \cite{Arzt2014}, p. 259-269.}
\textit{FlowDroid} extends the Java optimization framework \textit{Soot}, which was already used by \cite{Enck2011} for post-decompilation optimization tasks.\footnote{See \cite{Enck2011}, p. 5.}
The data flow is analysed by scanning an intermediate byte code format provided by \textit{Soot} for so called 'sources' and 'sinks'.\footnote{See \cite{Arzt2014}, p. 264.}
A source is the origin of a data flow, e.g. the user input of data via a textfield and a sink is the destination that data flows.
An example for a sink is a HTTP internet connection or a local log file.
\textit{FlowDroid} is also able to emulate Android lifecycle entry points.
While a regular Java program has a single entry point to start the application from, the \textit{main()} function, Android apps provide multiple entry points.
The entry points of an Android app are determined by the states an app can be in. 
It can e.g. return from being in the background, do a fresh start and return from being offline.
All these entry points are being emulated by \textit{FlowDroid} into a single \textit{main()} function call.
The output of the data flow analysis is one XML file per analysed APK file that contains a list of sinks and the coherent sources of data flows to that sink.
The XML file will be parsed by the main \sca tool later on and the sink and source methods will be interpreted in the context of information privacy risks.

The machine learning text classifiers will be trained within the second preparation step.
During the \sca phase of this study, we will be making great use of the naive Bayes classifier.
A machine learning text-classifier classifies text segments into distinct categories. 
The categories are predefined in the training phase of the classifier, since every trained text segment is assigned with a training category.
These training categories are the categories the classifier can assign to new, previously unseen, text segments.
The incisive feature of a Bayes classifier is the fact, that it chooses to classify a category to a new segment of text by picking the most probable category.\footnote{For this and the following two sentences see \cite{Rish2001}, p. 41.}
A \nbc furthermore assumes that all categories are distinct and independent of each other. 
Even though this might not always be the case in a real life usage scenario, the \nbc still performs well enough for a wide range of use cases.

In the case of the \sca in this study, we will be using the \nbc to classify URLs into categories.
The categories that URLs can belong to, in the context of this study, are: advertisement, delivery services, government, instant-messaging, (data-) aggregation services, search engines and social networks.
While the set of categories might not complete in terms of all possible and available categories, it is sufficient for the classification of URLs within this \sca to classify into the mentioned category-set.
In oder for the \nbc to classify text into categories we trained a \nbc implementation with meta-information about URLs from the previously mentioned categories.
First, it was necessary to collect URLs for the categories to train the \nbc and we used a collection of \acs{URL}s from \textit{URLBlacklist.com}\footnote{http://www.urlblacklist.com/?sec=download, visited 05/30/2016}.
\textit{URLBlacklist.com} provides URL lists for the categories advertisement, government, instant-messaging, search engines and social networks.
\textit{programmableweb.com} catalogues API descriptions including the service providers' URL.
We developed a program to automatically download and store the API directory for the two remaining categories, from \textit{programmableweb.com}.

Next, to acquire meta-information for all the URLs, we implement a downloader for the HTML source-code of all URLs and store the 'description' HTML-meta tag content in a file.
The 'description' meta-tag contains a small amount of text, provided by the website owner, that describes the content or function of the website topic.
We use this 'description' meta-information to train the classifier with the associated categories.\newline

As soon as the preparation steps are finished, the main \sca tool is ready to perform the \ipr analysis.
We call the main \sca tool '\textit{AIPRAT}', short for \aiprat from here on.
The fundamental concept of \AIPRAT is to iterate over all available apps and apply a set of analysis operations, called 'strategies', to the source code of these apps.
A strategy tries to identify \iprfs by applying algorithms, e.g. feature extraction or text search, to parts of the app source code.
There are two types of strategies in \AIPRAT, generic and specific strategies.

Generic strategies are strategies that contain algorithms that are configurable and usable by other specific strategies.
An example for a generic strategy is the Java-class \textit{analyze.src.strategies.ExistanceStrategy}. 
The \textit{ExistanceStrategy} is able to scan through all source code files of an app and search for a set of words.
If one or more of the source code files contains one or more of the search words, the \textit{ExistanceStrategy} returns a set of source code snippets that contain the lines of code containing the search words.
In total \AIPRAT contains eight generic strategies in the Java-package \textit{analyze.src.strategies}: DataFlowStrategy, ExistanceStrategy, InputStrategy, TraceBackStrategy, InformationCollectionStrategy, PermissionStrategy, ProviderUrlStrategy, and UrlCategoryStrategy.
The DataFlowStrategy parses the pre-extracted dataflow \acs{XML} from the \textit{FlowDroid} preparation phase and allows iterating over all identified dataflow sources and sinks.
Thereby, the DataFlowStrategy allows to pass parameters along that filter the sources and sinks for certain search words and provide feedback if the search words were found within sources and sinks.
To find strings within the source code of an app, one can make use of the ExistanceStrategy. 
This generic strategy scans the full source code of an app an collects source code lines that match a given search pattern.
The InputStrategy iterates over all \acs{XML} layout configuration files of an app. 
User interface controls that are displayed within an app are declared in these \acs{XML} layout configuration files.
The InputStrategy tries to identify all user input fields and therefore scans the layout files for the search terms 'EditText', 'AutoCompleteTextView', 'CheckBox' and 'RadioButton'.
As soon as a user input field has been found, the InputStrategy extracts all meta information about this input field possible.
The input fields meta information generally contain the user interface control 'id', a 'hint' field and a 'text' field. 
The meta information are collected and stored together with the input field information.

An important feature in static code analysis, especially for assessing information privacy risks, is to the ability to trace data flows from a source to a sink.
With the help of the call graph construction feature of FlowDroid, the TraceBackStrategy starts at a given set of start-sinks and traverses the call graph back until either a source is found or a given search pattern is matched.
This allows consequent strategies to define a search pattern for data flows to specific sinks.
In the case of this thesis, we will mainly use the TraceBackStrategy to find data flows that end in a information collection scenario. 
We define information collection as a data flow that results in storing the information either locally on the smart device the app is run, or that results in sending the information to a remote server.
With making use of the InputStrategy and the TracebackStrategy, the InputInformationCollectionStrategy takes the user input fields analysis one step further and allows for information collection analysis.
First, all user input fields are detected and stored. 
In a second step, the InputInformationCollectionStrategy executes a TraceBackStrategy that starts at all available information collection sinks (local file storage and remote server connections) and traces back the call graph in an attempt to identify the user input field 'ids' within the call graph path.
If a user input field 'id' is found, a data flow towards an information collection sink is identified and a potential \ipr revealed.
A less sophisticated approach is being used by the PermissionStrategy.
It is required for Android apps to declare permissions to use certain features, such as the \acs{GPS} location or internet access, within the 'manifest' file.
The PermissionStrategy enables a search through these permissions by a given search pattern.

The last two generic strategies concern the \acs{URL}s that are listed within the app source code and that are potentially target to information transfers.
The ProviderUrlStrategy iterates over all extracted URLs from the source code and checks how similar the URL host is in comparison to the app package name.
The package name is often similar to the hostname of the app provider or contains similar name parts. 
The ProviderUrlStrategy takes these potential sub-parts of the package name into account and returns a probability that a URL connection to the app provider is established.
Finally, the URLCategoryStrategy enables a search for a given category of URLs within the app source code.
All URLs are classified into distinct categories upon loading the app into the \sca tool via a machine learning text classification technique.
In order to check if a URL of a certain category exists, the iterates over all classified URLs and matches their categories to the search category.

A specific strategy, on the other hand, targets the exploration of an \ipp directly and contains the \ipp hierarchy identifier, introduced by \textcite{Dehling2016}, in its Java-classname.\footnote{See \cite{Dehling2016}, p. 6.}
The Java-class \textit{analyze.src.strategies.CI213\textunderscore Strategy} contains a search pattern for the \ipp with the hierarchy identifier CI213, which refers to the \ipp Content (C) $\rightarrow$ InformationCollectionContent (I) $\rightarrow$ InformationCollectionSensorContent (2) $\rightarrow$ EnvironmentSensorContent (1) $\rightarrow$ MicrophoneContent (3).
Therefore a specific strategy may contain a combination of one or many generic strategies to try to identify the risk of the associated \ipp is posing through static code analysis.
In the example of the specific strategy \textit{analyze.src.strategies.CI213\textunderscore Strategy}, the class extends the generic strategy class \textit{analyze.src.strategies.ExistanceStrategy} and sets the search parameters of the \textit{ExistanceStrategy} to 'MediaRecorder.setAudioSource(' and 'MediaRecorder.AudioSource.MIC'.
The \textit{CI213\textunderscore Strategy} class scans via the parent-class \textit{ExistanceStrategy} all of the source code files off the app for source code that uses the Android microphone \acs{API}.

\begin{table}

	\caption{Specific strategies in relation to the generic strategies they make use of.} 
	\begin{center}

	\begin{tabular}{ | p{4.8cm} | p{6cm} | }
	\hline
		DataFlowStrategy & CI221 \\ \hline
		ExistanceStrategy & CH23, CH310, CH312, CH33, CH35, CH37, CH38, CH43, CH44, CI212, CI213, CI214, CI221, CI222, CI223, CI231, CI242, CI243 \\ \hline
		InputInformationCollection-Strategy & CI321, CI322, CI323, CI324, CI325, CI326, CI341, CI343, CI344, CI345, CI346 \\ \hline
		InputStrategy & CI314 \\ \hline
		PermissionsStrategy & CH43, CH44, CI211, CI212, CI214, CI231 \\ \hline
		ProviderUrlStrategy & CH45 \\ \hline
		TraceBackStrategy & CH21, CH22, CH42, CH45, CI311, CI312, CI315, CI333, CI335 \\ \hline
		UrlCategoryStrategy & CH34, CH36, CH46 \\ \hline
	\end{tabular}
	
	\end{center}
\end{table}



% Evaluation of an Automated Information Privacy Risk Assessment Tool
\subsection{Evaluation of an Automated Information Privacy Risk Assessment Tool}

In order to evaluate how well the \aiprat is performing in comparison to a human researcher, we analyze a sample of three randomly chosen apps by two human researchers.
Each researcher is presented with the source code of the random apps and the list of relevant information privacy practices to identify.
The task for each researcher is to identify as many \ipp as possibly by analyzing, searching and reading through the source code files. 
This will result in an overview comparison on how well the \aiprat performs in comparison to a human.

We acknowledge that a human app reviewer can almost find any \ipp risk within the app source code given enough time.
In order to find an appropriate time range that the human review is allowed to spend analyzing a single app, we looked at common review times of app stores today.
The Google PlayStore does not manually review newly uploaded apps, while the Apple AppStore does apply manual review to each and every app update.
The average time it takes for an app to pass the Apple AppStore review  is currently two days.\footnote{See http://web.archive.org/web/20160513013009/http://appreviewtimes.com/, visited 05/13/2016}
In order to be comparibly competitive with the AppStore review times, a human app reviewer of our test sample apps should not spend more than 2 days on analyzing a single app.

The reviewers will document their results in a tabular form for each app.
For each of the relevant \ipp the reviewers will mark if they were able to detect the \ipr the \ipp poses or not.
Additionally they will report the certainty with which they feel they detected a certain information privacy practice.
The certainty levels are \textit{LOW}, \textit{MEDIUM}, \textit{HIGH}.

A \textit{HIGH} certainty expresses, that the reviewers are very confident that they found the \ipp in question.
A \textit{LOW} certainty indicates that the reviewers found a potential indicator for an \ipp but is rather uncertain if this is indication expresses the full \ipp or if there are not any other indicators for the \ipp that were not found yet or are unfindable.
To express a neither high nor low certainty about a \ipp found, the reviewers can use a \textit{MEDIUM} certainty level.